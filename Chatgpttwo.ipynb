{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"l2PX-8mRYyL8","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6bab8b1e-b34a-4a34-90e1-b605943c5042"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"]}],"source":["!pip install transformers\n","\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer\n","import torch\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Для наглядности будем работать с русскоязычной GPT от Сбера.\n","# Ниже команды для загрузки и инициализации модели и токенизатора.\n","model_name_or_path = \"sberbank-ai/rugpt3large_based_on_gpt2\"\n","tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n","model = GPT2LMHeadModel.from_pretrained(model_name_or_path).to(DEVICE)"]},{"cell_type":"code","source":["# prompt engineering for QA\n","text = 'Определение: \"Нейронная сеть\" - это'\n","input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(DEVICE)\n","out = model.generate(input_ids, do_sample=False)\n","\n","generated_text = list(map(tokenizer.decode, out))[0]\n","print(generated_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7twuxpmRlSkK","executionInfo":{"status":"ok","timestamp":1728819247308,"user_tz":-180,"elapsed":3730,"user":{"displayName":"Павел Петряев","userId":"06997609242486606883"}},"outputId":"91798fdf-a195-410a-fb71-52500f438101"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Определение: \"Нейронная сеть\" - это компьютерная программа, которая позволяет создавать и\n"]}]},{"cell_type":"code","source":["# Изначальные текст\n","text = \"я токен, придется меня токенизировать\"\n","# Процесс токенизации с помощьюю токенайзера ruGPT-3\n","tokens = tokenizer.encode(text, add_special_tokens=False)\n","# Обратная поэлементая токенизация\n","decoded_tokens = [tokenizer.decode([token]) for token in tokens]\n","\n","print(\"text:\", text)\n","print(\"tokens: \", tokens)\n","print(\"decoded tokens: \", decoded_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9S2jbH7gpjsJ","executionInfo":{"status":"ok","timestamp":1728819142924,"user_tz":-180,"elapsed":268,"user":{"displayName":"Павел Петряев","userId":"06997609242486606883"}},"outputId":"95c9d043-be73-4e09-f21a-71717a4adb7a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["text: я токен, придется меня токенизировать\n","tokens:  [274, 11016, 286, 16, 4128, 703, 11016, 337, 12703]\n","decoded tokens:  ['я', ' ток', 'ен', ',', ' придется', ' меня', ' ток', 'ени', 'зировать']\n"]}]},{"cell_type":"code","source":["text = 'from transformers import TextDataset, что это?'\n","input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(DEVICE)\n","# Пример вероятностного сэмплирования\n","out = model.generate(input_ids,\n","                     do_sample=True,\n","                     temperature=1.3,\n","                     top_k=20,\n","                     top_p=0.8,\n","                     max_length=30,\n","                    )\n","# Декодирование токенов\n","generated_text = list(map(tokenizer.decode, out))[0]\n","print(generated_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"-SDXnbx5p9QJ","executionInfo":{"status":"error","timestamp":1729429219918,"user_tz":-180,"elapsed":644,"user":{"displayName":"Павел Петряев","userId":"06997609242486606883"}},"outputId":"d79d7b16-e3cb-49e4-8597-f1b9b97e5fb0"},"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'tokenizer' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-f1227527bb03>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'from transformers import TextDataset, что это?'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Пример вероятностного сэмплирования\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m out = model.generate(input_ids,\n\u001b[1;32m      5\u001b[0m                      \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"]}]}],"metadata":{"colab":{"toc_visible":true,"provenance":[],"authorship_tag":"ABX9TyMttgNaMs1TD2Fr/PJdwnFc"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}